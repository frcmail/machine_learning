## 1. 特征缩放
实际当我们在计算线性回归模型的时候，会发现特征变量x，不同维度之间的取值范围差异很大。这就造成了我们在使用梯度下降算法的时候，由于维度之间的差异使得Jθ的值收敛的很慢。

我们还是以房价预测为例子，我们使用2个特征。房子的尺寸（1~2000），房间的数量（1-5）。以这两个参数为横纵坐标，绘制代价函数的等高线图能看出
整个图显得很扁，假如红色的轨迹即为函数收敛的过程，会发现此时函数收敛的非常慢。

![](http://img.blog.csdn.net/20160418193311664)

为了解决这个问题，我们采用`特征缩放`,所谓的特征缩放就是把所有的特征都缩放到 -1 ~ 1之间。最简单的方法采用下面的公式进行计算

![](http://img.blog.csdn.net/20160418193448508)

1. Xn表示第n个特征，也就是特征变量X的第n维
2. Un表示特征的平均值，也就是第n个特征的平均值
3. Sn表示标准差，也就是第n个特征中最大值和最小值的差

## 2. 学习速率
到目前为止，我们还没有介绍如何选择学历速率α，梯度下降算法每次迭代，都会受到学习速率α的影响
1. 如果α较小，则达到收敛所需要迭代的次数就会非常高；
2. 如果α较大，则每次迭代可能都不会减小代价函数的结果，甚至会超过局部最小值导致无法收敛。
   ![](https://camo.githubusercontent.com/1b6f2c394d39a7c057a5726c7e1b3ce6ee9c6362/687474703a2f2f696d672e6d792e6373646e2e6e65742f75706c6f6164732f3230313230392f30362f313334363930323330305f343137392e706e67)

根据经验，可以从以下几个数值开始试验α的值，0.001,0.003,0.01,0.03,0.1,0.3,1,… 
α初始值位0.001, 不符合预期乘以3倍用0.003代替，不符合预期再用0.01替代，如此循环直至找到最合适的α

