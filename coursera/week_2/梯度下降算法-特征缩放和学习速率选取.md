## 1. 特征缩放
实际当我们在计算线性回归模型的时候，会发现特征变量x，不同维度之间的取值范围差异很大。这就造成了我们在使用梯度下降算法的时候，由于维度之间的差异使得Jθ的值收敛的很慢。

我们还是以房价预测为例子，我们使用2个特征。房子的尺寸（1~2000），房间的数量（1-5）。以这两个参数为横纵坐标，绘制代价函数的等高线图能看出
整个图显得很扁，假如红色的轨迹即为函数收敛的过程，会发现此时函数收敛的非常慢。

![](http://img.blog.csdn.net/20160418193311664)

为了解决这个问题，我们采用`特征缩放`,所谓的特征缩放就是把所有的特征都缩放到 -1 ~ 1之间。最简单的方法采用下面的公式进行计算

![](http://img.blog.csdn.net/20160418193448508)

1. Xn表示第n个特征，也就是特征变量X的第n维
2. Un表示特征的平均值，也就是第n个特征的平均值
3. Sn表示标准差，也就是第n个特征中最大值和最小值的差

## 2. 学习速率
到目前为止，我们还没有介绍如何选择学历速率α，梯度下降算法每次迭代，都会受到学习速率α的影响

1. 如果α较小，则达到收敛所需要迭代的次数就会非常高；
2. 如果α较大，则每次迭代可能都不会减小代价函数的结果，甚至会超过局部最小值导致无法收敛。
   ![](https://camo.githubusercontent.com/1b6f2c394d39a7c057a5726c7e1b3ce6ee9c6362/687474703a2f2f696d672e6d792e6373646e2e6e65742f75706c6f6164732f3230313230392f30362f313334363930323330305f343137392e706e67)

观察下图，可以发现这三种情况下代价函数 J(θ)的迭代都不是正确的

![](http://images.cnitblog.com/blog/663864/201410/272201153783110.png)

1. 第一个图，曲线在上升，明显J(θ)的值变得越来越大，说明应该选择较小的α
2. 第二个图，J(θ)的曲线，先下降，然后上升，接着又下降，然后又上升，如此往复。通常解决这个问题，还是选取较小的α

根据经验，可以从以下几个数值开始试验α的值，0.001,0.003,0.01,0.03,0.1,0.3,1,…

α初始值位0.001, 不符合预期乘以3倍用0.003代替，不符合预期再用0.01替代，如此循环直至找到最合适的α

然后对于这些不同的 α 值，绘制 J(θ)随迭代步数变化的曲线，然后选择看上去使得 J(θ)快速下降的一个 α 值。

所以，在为梯度下降算法选择合适的学习速率时，可以大致按3的倍数来取值一系列α值，直到我们找到一个值它不能再小了，同时找到另一个值，它不能再大了。其中最大的那个 α 值，或者一个比最大值略小一些的合理的值。


