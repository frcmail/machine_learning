## 1.引言
假设我们的训练集如下图1所示，因此我们可以假设hypothesis函数如图2

![](https://camo.githubusercontent.com/f5198c4c49fed874a5c471187c9327c4281527b2/687474703a2f2f696d616765732e636e6974626c6f672e636f6d2f626c6f672f3339323232382f3230313431302f3239313931393431303635353830352e6a7067)

![](http://studentdeng.github.io/images/ml/1.png)
θ0和θ1我们称为hθ(x)函数的2个参数，h是x的函数，所以有时候也记着h(x)

对于这个已有的hypothesis，我们需要什么方法来评估这个假设函数的好坏呢？

因此我们定义了一个叫"代价函数"cost function 来评估当前hθ(x)函数

## 2. 代价函数
cost function也叫作loss function，就是对hθ(x)函数进行评估的一个函数。

通常来说，模型越准确，越接近真实，其cost function的值就越小。

cost function 通常用大写字母J表示，由于cost function的大小和hθ(x)的参数取值相关，不难想象，J是θ的函数，用Jθ表示。

线性回归中的cost function，通常用最小"平方差"来表示，也称为square loss，定义函数如下所以

![](https://camo.githubusercontent.com/9b486198032d4371e83f37c39d0dbec6e12013dd/687474703a2f2f73747564656e7464656e672e6769746875622e696f2f696d616765732f6d6c2f31322e706e67)


这里x和y的右上角标i，表示training set中第i个数据的特征向量和实际值，加括号表示是第i个，而不是i次幂。

通常，也有如下的表示形式：

Jθ=12m(hθ(x(i))−y(i))2

这样做是方便后面求偏导数的时候约掉偏导数系数中的2，更方便计算。

由loss function 可以得知，当hypothesis 对training set测试结果完全正确的时候，loss function的值是0。

而我们的目标，就是尽量优化hypothesis，不断更改θ
的值，使得Jθ

的值最小，即

MinimizeθJ(θ)

对Cost Function的解读

至此，cost function仍稍显抽象，好在Linear Regression问题的cost function 是一个二次函数，图像是好画的，我们尝试从图像理解cost function和对cost function 求解权重向量的目标。

下面我们以一元一次回归方程的cost function
Jθ=12m(hθ(x(i))−y(i))2

为例。

我们注意到J(θ)
是一个二次函数，cost function有两个未知数θ0和θ1，固定一个参，θ0时，J(θ)是

\theta_1$的二次函数，其图像应该是一个抛物线。可能长得如下图所示：

cost function一个分量的图像

注意到曲线的全局极值是大于等于0的，当且仅当training set能够完美拟合成一条直线的时候，抛物线的极值才会触到x轴上。

类似的，J(θ)
和θ0

的图像也是这样一条抛物线曲线。

如果综合两个参数θ0
和θ1

，其曲线可能长这个样子：

cost function的图像

其全局极值点的坐标，就是我们要求的权重向量θ

。
阶段总结

到现在为止，我们已经把一个问题抽象成机器学习喜闻乐见的形式：

Linear Regression的问题描述

    一般地，对于机器学习问题，我们都要这样，确定hypothesis，cost function和goal，通常对cost function 求解权重向量的过程，是一个最优化问题。
