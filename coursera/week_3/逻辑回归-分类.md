## 1. 逻辑回归
监督学习中另一个问题为分类问题，常见的分类问题例子有

1. 邮件是否是垃圾邮件，0表示垃圾邮件，1表示正常邮件
2. 在线交易是否会欺骗用户，0表示会欺骗，1表示不会
3. 患肿瘤患者是良性还是恶性，0表示恶性，1表示良性

这些问题，可以归之于`二分类问题`，y表示因变量，可以定义如下

![](http://52opencourse.com/?qa=blob&qa_blobid=8298951987645335658)

其中0表示负例，1表示正例
同理，对于多分类问题来说，因变量y的值可以取{0,1,2,3 ... n}

我们先从`二分类问题`入手，理解什么是逻辑回归分类

逻辑回归（Logistic Regression）是一种用于解决二分类问题（0 or 1）的机器学习模型
逻辑回归（Logistic Regression）与线性回归（Linear Regression）都是一种广义线性模型（generalized linear model）逻辑回归假设因变量y 服从`伯努利分布`，而线性回归假设因变量y服从`高斯分布`

## 2. 逻辑回归模型
线性回归模型中，假设函数h(x)预测的值是连续的，值域在`[负无穷, 正无穷]`。
所以线性回归模型假设函数h(x)无法满足逻辑回归模型，因此我们引入了函数g, 同时假设逻辑回归模型的假设函数h(x)如下

![](http://52opencourse.com/?qa=blob&qa_blobid=10421873245604717587)

这个函数称为Sigmoid函数，也称为逻辑函数（Logistic function） 函数曲线如下

![](http://img.blog.csdn.net/20160409203837285)

从上图可以看到逻辑函数是一个s形的曲线，它的取值在[0,1]之间

因此，对于逻辑回归的假设函数h(x)，预测得到的值满足 0 <= h(x) <= 1

所以我们需要一个边界，把h(x)最终预测的值y映射为0或1

我们可以设置一个阈值d，当h(x) < d的时候映射为0，当h(x) >= d的时候映射为1

假设阀值d = 0.5

1. 当h(x) >= 0.5的时候，则预测y=1, 既y属于正例
2. 当h(x) < 0.5的时候，则预测y=0, 既y属于负例


## 3. 决策边界
假设阀值d=0.5, 当h(x) >= 0.5的时候，预测y=1
从逻辑函数的曲线图可知，对于函数h(x) = g(theta^T * x) >= 0.5，等价于theta^T * x >= 0

因此，我们可以称 `theta^T * x = 0` 是`决策边界`，大于0的时候预测值为1，小于0的时候预测值为0.

举个例子
假设h(x) = g(theta0 + theta1*x1 + theta2*x2)，theta0, theta1, theta2 分别取-3, 1, 1
由上可知，决策边界 `-3 + x1 + x2 = 0` 是一个线性的方程，如下图所示

![](http://52opencourse.com/?qa=blob&qa_blobid=17042785878272231122)

当h(x)更加复杂的时候，决策边界可能是一个非线性的方程

![](http://52opencourse.com/?qa=blob&qa_blobid=6779199348343391320)

![](http://52opencourse.com/?qa=blob&qa_blobid=14854555397734057469)

