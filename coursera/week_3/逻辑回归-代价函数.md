## 1. 引言
回到线性回归模型中，训练集和代价函数如下图

![](http://images.cnitblog.com/blog/575572/201311/09081133-fe9c33298fe44030b121be27a7d2d493.png)

![](https://camo.githubusercontent.com/69d7473a15e3ebc5f447bdf7d3091cc2eb0a4f8e/687474703a2f2f696d672e626c6f672e6373646e2e6e65742f3230313630343138313931333030333836)

如果我们还用J(θ)函数做为逻辑回归模型的代价函数，用H(x) = g(θ^T * x) 发现J(θ)的曲线图是"非凸函数"，存在多个局部最小值，不利于我们求解全局最小值

![](http://52opencourse.com/?qa=blob&qa_blobid=607435295049781725)

因此，上述的`代价函数`对于逻辑回归是不可行的，我们需要其他形式的`代价函数`来保证逻辑回归的`代价函数`是凸函数。

## 2. 代价函数
这里我们先对线性回归模型中的代价函数J(θ)进行简单的改写

![](http://images.cnitblog.com/blog/575572/201311/09081405-9b492cc9537d4e6bb4a979aaf640e862.png)

![](http://images.cnitblog.com/blog/575572/201311/09081437-30ae997c4ec8401a9daf276cede74bc7.png)




